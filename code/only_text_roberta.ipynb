{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5536b-8df4-441a-bf78-d829af77688c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install multimodal-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32382d4-2263-4792-b959-55c6bdcde384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --user datasets\n",
    "pip install --user openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9876cc3-ddb6-4fe8-bfd5-60ca62934a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user tensorboard\n",
    "pip install --user tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82246e1-c4ff-4d4a-923d-fc495c91d866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, Trainer, EvalPrediction, set_seed\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "import sys\n",
    "\n",
    "from multimodal_transformers.data import load_data_from_folder\n",
    "from multimodal_transformers.model import TabularConfig\n",
    "from multimodal_transformers.model import AutoModelWithTabular\n",
    "from multimodal_transformers.multimodal_arguments import (\n",
    "    ModelArguments,\n",
    "    MultimodalDataTrainingArguments,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ[\"COMET_MODE\"] = \"DISABLED\"\n",
    "# print(multimodal_transformers.getsitepackages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f02ce-33b4-443b-84ec-6b157697330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = [\"GPT4O-Contribution\"]\n",
    "cat_cols =[]\n",
    "numerical_cols = [\"Hit_1pct\",'Hit_5pct','Hit_10pct','Atyp_10pct_Z','Atyp_Median_Z',\n",
    "                  'Atyp_Pairs','C10','C5','C_f','Citation_Count','NCT_Count','NSF_Count',\n",
    "                  'Newsfeed_Count','Patent_Count','Reference_Count','SB_B','SB_T','Team_Size',\n",
    "                  'Tweet_Count','WSB_Cinf','WSB_sigma','cit_d','important_cit_per','ref_5_per',\n",
    "                  'ref_avg_age','ref_cit_mean','ref_d','ref_median_age']\n",
    "\n",
    "column_info_dict = {\n",
    "    \"text_cols\": text_cols,\n",
    "    \"num_cols\": numerical_cols,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"label_col\": \"B/NB\",\n",
    "    \"label_list\": [\"NB\", \"IB\", \"B\"],\n",
    "}\n",
    "model_args = ModelArguments(model_name_or_path=\"roberta-base\")\n",
    "\n",
    "# weighted_feature_sum_on_transformer_cat_and_numerical_feats\n",
    "# attention_on_cat_and_numerical_feats\n",
    "#gating_on_cat_and_num_feats_then_sum\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_path=\"./only_text_robert\",\n",
    "    combine_feat_method=\"text_only\",\n",
    "    column_info=column_info_dict,\n",
    "    task=\"classification\",\n",
    "    categorical_encode_type=None,\n",
    "    categorical_handle_na=True,\n",
    "    categorical_na_value=\"Unknown\",\n",
    "    ohe_handle_unknown=\"error\",\n",
    "    numerical_transformer_method='none',\n",
    "    numerical_handle_na=True,\n",
    "    numerical_how_handle_na=\"medium\",\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./only_text_robert\",\n",
    "    logging_dir=\"./only_text_robert/log\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #seed=42,\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=100,\n",
    "    evaluation_strategy=\"epoch\",  # 每个 epoch 进行评估\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True,  # 加载最佳模型\n",
    "    metric_for_best_model='f1',  # 选择用于比较的指标\n",
    "    logging_steps=25,\n",
    "    eval_steps=250,\n",
    "    greater_is_better=True  # 选择的指标越大越好\n",
    ")\n",
    "print(training_args.seed)\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee8649-4d5d-4573-9827-e18a26e7935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path_or_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
    "print('Specified tokenizer: ', tokenizer_path_or_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path_or_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    truncation=True,     # 自动截断超过 max_length 的序列\n",
    "    max_length=512,      # 设置最大序列长度为 512\n",
    "    padding='max_length'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077e55f-80bf-42fe-acbd-0ccae10a3d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Datasets\n",
    "train_dataset, val_dataset, test_dataset = load_data_from_folder(\n",
    "    data_args.data_path,\n",
    "    data_args.column_info[\"text_cols\"],\n",
    "    tokenizer,\n",
    "    label_col=data_args.column_info[\"label_col\"],\n",
    "    label_list=data_args.column_info[\"label_list\"],\n",
    "    categorical_encode_type = data_args.categorical_encode_type,\n",
    "    numerical_transformer_method = data_args.numerical_transformer_method,\n",
    "    categorical_cols=data_args.column_info[\"cat_cols\"],\n",
    "    numerical_cols=data_args.column_info[\"num_cols\"],\n",
    "    sep_text_token_str=tokenizer.sep_token,\n",
    "    max_token_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92dd31-eb75-4ba1-980c-4e0b308230bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(np.unique(train_dataset.labels))\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871bec7e-4b7d-4770-b5b6-cc2b4d57287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tabular_config = TabularConfig(\n",
    "    num_labels=num_labels,\n",
    "    #cat_feat_dim=train_dataset.cat_feats.shape[1],\n",
    "    numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n",
    "    **vars(data_args)\n",
    ")\n",
    "config.tabular_config = tabular_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c194e-fd72-4346-80b2-263ab20060ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelWithTabular.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e95e1f1-77ce-4f11-898d-02b02a5f23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "\n",
    "def calc_classification_metrics(p: EvalPrediction):\n",
    "    predictions = p.predictions[0]\n",
    "    pred_labels = np.argmax(predictions, axis=1)\n",
    "    pred_scores = softmax(predictions, axis=1)[:, 1]\n",
    "    labels = p.label_ids\n",
    "    if len(np.unique(labels)) == 2:  # binary classification\n",
    "        roc_auc_pred_score = roc_auc_score(labels, pred_scores)\n",
    "        precisions, recalls, thresholds = precision_recall_curve(labels, pred_scores)\n",
    "        fscore = (2 * precisions * recalls) / (precisions + recalls)\n",
    "        fscore[np.isnan(fscore)] = 0\n",
    "        ix = np.argmax(fscore)\n",
    "        threshold = thresholds[ix].item()\n",
    "        pr_auc = auc(recalls, precisions)\n",
    "        tn, fp, fn, tp = confusion_matrix(labels, pred_labels, labels=[0, 1]).ravel()\n",
    "        result = {\n",
    "            \"roc_auc\": roc_auc_pred_score,\n",
    "            \"threshold\": threshold,\n",
    "            \"pr_auc\": pr_auc,\n",
    "            \"recall\": recalls[ix].item(),\n",
    "            \"precision\": precisions[ix].item(),\n",
    "            \"f1\": fscore[ix].item(),\n",
    "            \"tn\": tn.item(),\n",
    "            \"fp\": fp.item(),\n",
    "            \"fn\": fn.item(),\n",
    "            \"tp\": tp.item(),\n",
    "        }\n",
    "    else:\n",
    "        # [None, 'micro', 'macro', 'weighted']\n",
    "        acc = (pred_labels == labels).mean()\n",
    "        f1 = f1_score(y_true=labels, y_pred=pred_labels, average='weighted')\n",
    "        result = {\n",
    "            \"acc\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"acc_and_f1\": (acc + f1) / 2,\n",
    "            \"mcc\": matthews_corrcoef(labels, pred_labels),\n",
    "\n",
    "        }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e39661-53b0-4486-aa2e-f60421260754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # 3个评估周期内无改进则停止\n",
    "    early_stopping_threshold=0.0001  # 改进幅度必须超过1%\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=calc_classification_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f329563-ce42-4c7a-9402-e72f09e51d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75749e-ad90-4c7e-a629-658b460ab866",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.evaluate(eval_dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c79be-7649-4c3a-be4f-5b9cc2ee658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 确保模型在 cuda:0 上\n",
    "model.to(device)\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "# 确保所有输入张量都在同一设备上\n",
    "# test_batch = {k: v.to(device) for k, v in test_batch.items()}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_loader:\n",
    "        test_batch = {k: v.to(device) for k, v in test_batch.items()}\n",
    "        _, logits, classifier_outputs = model(\n",
    "            test_batch[\"input_ids\"],\n",
    "            attention_mask=test_batch[\"attention_mask\"],\n",
    "            # token_type_ids=test_batch[\"token_type_ids\"],\n",
    "            cat_feats=test_batch[\"cat_feats\"],\n",
    "            numerical_feats=test_batch[\"numerical_feats\"],\n",
    "        )\n",
    "        all_logits.append(logits)\n",
    "        labels = test_batch[\"labels\"].to(device)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# 确保 labels 在同一设备上\n",
    "all_logits = torch.cat(all_logits, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)  # 合并标签列表为一个张量\n",
    "\n",
    "# 计算预测类别\n",
    "predictions = all_logits.argmax(axis=1)\n",
    "\n",
    "# 计算指标\n",
    "accuracy = (predictions == all_labels).float().mean().item()\n",
    "precision = precision_score(all_labels.cpu(), predictions.cpu(), average='weighted')\n",
    "recall = recall_score(all_labels.cpu(), predictions.cpu(), average='weighted')\n",
    "f1 = f1_score(all_labels.cpu(), predictions.cpu(), average='weighted')\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c55cd-996f-4f03-849e-bf4378922973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
